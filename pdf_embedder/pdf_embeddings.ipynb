{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0abb94-9e9b-40b5-bb83-6e08f85fc047",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb1d98f-f34b-4092-b08d-a38a1f8aab9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, uuid\n",
    "import numpy as np\n",
    "from InstructorEmbedding import INSTRUCTOR\n",
    "from PyPDF2 import PdfReader\n",
    "import re\n",
    "import unicodedata\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14aeb83-a714-4c8b-9026-7b71505e2c51",
   "metadata": {},
   "source": [
    "## Text processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e3ca1e-b35f-4878-aa2f-460207cf23ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf_text(pdf_path: str) -> str:\n",
    "    reader = PdfReader(pdf_path)\n",
    "    pages = []\n",
    "    for p in reader.pages:\n",
    "        pages.append(p.extract_text() or \"\")\n",
    "    return \"\\n\".join(pages)\n",
    "\n",
    "\n",
    "def normalize_unicode(s: str) -> str:\n",
    "    s = unicodedata.normalize(\"NFKC\", s)\n",
    "    # normalize curly quotes and long dashes to ASCII\n",
    "    s = s.replace(\"â€œ\", '\"').replace(\"â€\", '\"').replace(\"â€˜\", \"'\").replace(\"â€™\", \"'\")\n",
    "    s = s.replace(\"â€“\", \"-\").replace(\"â€”\", \"-\")\n",
    "    return s\n",
    "\n",
    "\n",
    "def dehyphenate_linebreaks(s: str) -> str:\n",
    "    # Join words split across line breaks: e.g. \"classi-\\nfication\" -> \"classification\"\n",
    "    return re.sub(r\"(\\w)-\\s*\\n\\s*(\\w)\", r\"\\1\\2\", s)\n",
    "\n",
    "\n",
    "def collapse_linebreaks(s: str) -> str:\n",
    "    # Convert hard line breaks inside paragraphs to spaces, keep paragraph breaks\n",
    "    # 1) normalize Windows/Mac line endings\n",
    "    s = s.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    # 2) keep blank-line paragraph breaks, but turn single newlines into spaces\n",
    "    s = re.sub(r\"[ \\t]*\\n[ \\t]*(?=[^\\n])\", \" \", s)                  # single linebreak -> space\n",
    "    s = re.sub(r\"(?:\\n\\s*){2,}\", \"\\n\\n\", s)                          # 2+ linebreaks -> exactly two\n",
    "    return s\n",
    "\n",
    "\n",
    "def remove_page_furniture(s: str) -> str:\n",
    "    # Remove typical headers/footers and page number lines\n",
    "    patterns = [\n",
    "        r\"^\\s*page\\s+\\d+(\\s+of\\s+\\d+)?\\s*$\",                         # Page X (of Y)\n",
    "        r\"^\\s*this project is co[- ]funded.*$\",                      # common boilerplate\n",
    "        r\"^\\s*centre for humanitarian data.*$\",                      # example header\n",
    "        r\"^\\s*european union.*$\",                                    # example footer\n",
    "        r\"^\\s*acknowledg(e)?ments.*$\",                               # section heading noise (optional)\n",
    "    ]\n",
    "    rx = re.compile(\"|\".join(f\"(?:{p})\" for p in patterns), re.I | re.M)\n",
    "    return rx.sub(\"\", s)\n",
    "\n",
    "\n",
    "def fix_ocr_letter_spacing(s: str) -> str:\n",
    "    # Collapse spaced-out OCR words like \"S O M A L I A\" -> \"Somalia\"\n",
    "    # Heuristic 1: 3+ single-letter tokens separated by spaces\n",
    "    def _collapse(match):\n",
    "        word = match.group(0).replace(\" \", \"\")\n",
    "        # Capitalize if it looked like a title word (mostly uppercase)\n",
    "        if word.isupper():\n",
    "            return word.title()\n",
    "        return word.lower()\n",
    "    s = re.sub(r\"\\b(?:[A-Za-z]\\s){2,}[A-Za-z]\\b\", _collapse, s)\n",
    "\n",
    "    # Heuristic 2: handle inconsistent spacing inside mid-length words: \"re c las s if ied\"\n",
    "    # Collapse space between letters when surrounded by letters on both sides\n",
    "    s = re.sub(r\"(?<=\\w)\\s+(?=\\w)\", \" \", s)  # first normalize to single spaces\n",
    "    s = re.sub(r\"\\b([A-Za-z])\\s(?=[A-Za-z])\", r\"\\1\", s)  # then remove residual intra-word single spaces\n",
    "    return s\n",
    "\n",
    "\n",
    "def remove_inline_refs_urls_emails(s: str) -> str:\n",
    "    # Remove bracketed numeric refs like [12], (12)\n",
    "    s = re.sub(r\"\\s*\\[\\d+\\]\", \"\", s)\n",
    "    s = re.sub(r\"\\s*\\(\\d+\\)\", \"\", s)\n",
    "    # Remove raw URLs/emails (keep if you actually need them)\n",
    "    s = re.sub(r\"\\bhttps?://\\S+\\b\", \"\", s, flags=re.I)\n",
    "    s = re.sub(r\"\\bwww\\.\\S+\\b\", \"\", s, flags=re.I)\n",
    "    s = re.sub(r\"\\b\\S+@\\S+\\.\\S+\\b\", \"\", s)\n",
    "    return s\n",
    "\n",
    "\n",
    "def tidy_spaces_punctuation(s: str) -> str:\n",
    "    # Remove non-printable/control chars\n",
    "    s = re.sub(r\"[^\\x09\\x0A\\x0D\\x20-\\x7E]\", \" \", s)\n",
    "    # Collapse multiple spaces\n",
    "    s = re.sub(r\"[ \\t]{2,}\", \" \", s)\n",
    "    # Remove spaces before punctuation\n",
    "    s = re.sub(r\"\\s+([,.;:?!%])\", r\"\\1\", s)\n",
    "    # Ensure single space after punctuation where appropriate\n",
    "    s = re.sub(r\"([,.;:?!%])([^\\s])\", r\"\\1 \\2\", s)\n",
    "    # Trim lines and trailing spaces around paragraphs\n",
    "    s = re.sub(r\"[ \\t]+(\\n)\", r\"\\1\", s)\n",
    "    return s.strip()\n",
    "\n",
    "\n",
    "def dedupe_paragraphs(s: str) -> str:\n",
    "    # Drop exact duplicate paragraphs (simple but effective)\n",
    "    paras = [p.strip() for p in s.split(\"\\n\\n\")]\n",
    "    seen, out = set(), []\n",
    "    for p in paras:\n",
    "        if p and p not in seen:\n",
    "            seen.add(p)\n",
    "            out.append(p)\n",
    "    return \"\\n\\n\".join(out)\n",
    "\n",
    "\n",
    "\n",
    "def clean_text(raw: str) -> str:\n",
    "    text = normalize_unicode(raw)\n",
    "    text = dehyphenate_linebreaks(text)\n",
    "    text = collapse_linebreaks(text)\n",
    "    text = remove_page_furniture(text)\n",
    "    text = fix_ocr_letter_spacing(text)\n",
    "    text = remove_inline_refs_urls_emails(text)\n",
    "    text = tidy_spaces_punctuation(text)\n",
    "    text = dedupe_paragraphs(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def process_text_by_pages(folder_path):\n",
    "    count = 0\n",
    "    all_pdf_chunks = {}  # {filename: [page1_text, page2_text, ...]}\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if count >= 2:\n",
    "            break\n",
    "\n",
    "        if filename.lower().endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(folder_path, filename)\n",
    "            count += 1\n",
    "\n",
    "            reader = PdfReader(pdf_path)\n",
    "            page_chunks = []\n",
    "\n",
    "            for page_number, page in enumerate(reader.pages, start=1):\n",
    "                raw_text = page.extract_text() or \"\"\n",
    "                cleaned_page = clean_text(raw_text)\n",
    "                page_chunks.append({\n",
    "                    \"page_number\": page_number,\n",
    "                    \"text\": cleaned_page\n",
    "                })\n",
    "\n",
    "            all_pdf_chunks[filename] = page_chunks\n",
    "            print(f\"Processed {filename} into {len(page_chunks)} page chunks.\")\n",
    "\n",
    "    return all_pdf_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15718cd-4cd2-400a-9b3a-5ea10497cf67",
   "metadata": {},
   "source": [
    "## Embedding Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e7489a-e13f-462d-8cb8-2177f741c749",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_embed(texts):\n",
    "    model = INSTRUCTOR(\"hkunlp/instructor-xl\")\n",
    "    instruction = \"Represent the sentence for semantic search\"\n",
    "\n",
    "    pairs = [[instruction, t] for t in texts]\n",
    "    embs = model.encode(pairs, normalize_embeddings=True)  # cosine-friendly\n",
    "    embs = np.asarray(embs, dtype=\"float32\")\n",
    "    dim = embs.shape[1]\n",
    "\n",
    "    return dim,embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f8d2e8-c8bb-4d29-8366-0829fc000955",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "\n",
    "folder_path = str(os.environ.get(\"PDF_FOLDER_PATH\"))\n",
    "\n",
    "pdf_page_chunks = process_text_by_pages(folder_path)\n",
    "# print(pdf_page_chunks[0])\n",
    "# Preview first PDF's first chunk\n",
    "count = 1\n",
    "for filename, pages in pdf_page_chunks.items():\n",
    "    print(f\"\\nðŸ“„ {filename} - {len(pages)} pages\")\n",
    "    for p in pages:  # show only first 2 pages\n",
    "        print(f\"--- Page {p['page_number']} ---\")\n",
    "        texts.append({\"text\":p['text']})\n",
    "        count += 1\n",
    "        \n",
    "\n",
    "# print(texts[0])\n",
    "# print(texts[6])\n",
    "embedded_text = text_embed(texts)\n",
    "print(embedded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff9b08f-fb72-441e-839c-e582537ddb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = []\n",
    "for i, emb in enumerate(embedded_text[1]):\n",
    "    vectors.append((\n",
    "        f\"id-{i}\",            # unique ID\n",
    "        emb.tolist(),         # convert to list\n",
    "        {\"text\": texts[i][\"text\"]}    # optional metadata\n",
    "    ))\n",
    "# print(vectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b84a6e3-05eb-4e98-96fd-0bb10c615ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9726e2-ba66-4c0a-8a0c-fa3f6e83dab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = Pinecone(api_key = os.environ.get(\"PINECONE_API_KEY\"), environment = os.environ.get(\"PINECONE_ENV\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374395b3-b9c2-4acf-905c-fdb70aedc2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.list_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df618e43-de44-4d76-9fc0-2f386438fc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"pdf-test\"\n",
    "dimension = 1024\n",
    "metric = \"cosine\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e34797e-e3c9-44c4-b496-f5064fd6482f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if index_name in [index.name for index in pc.list_indexes()]:\n",
    "    pc.delete_index(index_name)\n",
    "    print(f\"{index_name} succesfully deleted.\")\n",
    "else:\n",
    "     print(f\"{index_name} not in index list.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462e2897-7a6e-49c7-a911-bfdbc7d91b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.create_index(\n",
    "    name = index_name, \n",
    "    dimension = dimension, \n",
    "    metric = metric, \n",
    "    spec = ServerlessSpec(\n",
    "        cloud = \"aws\", \n",
    "        region = \"us-east-1\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be41d80-a2d3-483d-b158-2a6a7edab6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc88e51-8dc2-4098-80c4-4a7b912f127c",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.upsert(vectors = vectors)\n",
    "print(\"âœ… Inserted embeddings into Pinecone\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_env",
   "language": "python",
   "name": "langchain_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
